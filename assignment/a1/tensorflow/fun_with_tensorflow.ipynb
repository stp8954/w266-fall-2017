{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Run thie cell to import everything we'll need.\n",
        "import tensorflow as tf\n",
        "import graph\n",
        "import graph_test\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import unittest\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "reload(graph)\n",
        "reload(graph_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fun with TensorFlow (10 points)\n",
        "\n",
        "The goal of this section is to familiarize yourself with the Python [TensorFlow API](https://www.tensorflow.org/api_docs/python/index.html). We'll be using TensorFlow throughout the class to implement deep learning models, which are the state-of-the-art on many NLP tasks such as machine translation, sentiment analysis, and language modeling.\n",
        "\n",
        "### TensorFlow: Declarative Numerical Programming\n",
        "\n",
        "The TensorFlow programming model has two phases:\n",
        "1.  **Construct a graph** by running Python code\n",
        "2.  **Execute the graph** by calling `session.run`\n",
        "\n",
        "In the **graph construction** phase, we operate on everything symbolically. Executing the Python code doesn't actually do any numerical calculations - it just tells TensorFlow how to do the computation later. Every variable you define here is a **Tensor**, which creates a node in the computation graph.\n",
        "\n",
        "In the **execution phase**, we give TensorFlow input data and a list of output operations. It runs the data through the graph and returns numerical results as NumPy arrays.\n",
        "\n",
        "#### Tensor Objects\n",
        "\n",
        "Tensor objects are the symbolic equivalent of NumPy arrays, and support many similar operations. For example, to compute a linear model $y = vW + b$ in NumPy, you might do:\n",
        "```python\n",
        "# w, v are np.ndarray\n",
        "y = np.dot(v, w) + b\n",
        "```\n",
        "In TensorFlow, this would be expressed as:\n",
        "```python\n",
        "# w, v, b are tf.Tensor\n",
        "y = tf.matmul(v, w) + b\n",
        "```\n",
        "\n",
        "There are a few ways to define Tensors, but the most important are:\n",
        "\n",
        "- **[Constants and sequences](https://www.tensorflow.org/versions/r0.10/api_docs/python/constant_op.html#constants-sequences-and-random-values)**, like tf.constant(), tf.zeros(), or tf.linspace(). These create a Tensor with a fixed value, and pretty much work like their NumPy equivalents.\n",
        "\n",
        "- **[Variables](https://www.tensorflow.org/versions/r0.10/how_tos/variables/index.html)**, which are persistent and can be modified during execution. Think model parameters, which get updated by training.\n",
        "\n",
        "- **[Placeholders](https://www.tensorflow.org/versions/r0.10/api_docs/python/io_ops.html#placeholder)**, which are used for data inputs. You feed these in by passing a NumPy array at execution time.\n",
        "\n",
        "Operations on tensors - like `tf.matmul()` or `tf.nn.softmax()` - produce other tensors and add additional nodes to the graph.\n",
        "\n",
        "#### Delayed Execution\n",
        "\n",
        "The key difference between the NumPy code `y = np.dot(v, w) + b` and the TensorFlow equivalent `y = tf.matmul(v, w) + b` is that the latter _doesn't actually do the computation_. Instead, it tells TensorFlow that `y` is derived by performing the `matmul` operation on `v` and `w`, followed by adding `b`. In order to crunch the numbers, you need to run the graph, such as:\n",
        "```python\n",
        "# w, b defined as persistent tf.Variable, assume w is 10-dimensional vector\n",
        "y = tf.matmul(v, w) + b  # Add Op (Tensor) to the graph\n",
        "y_value = session.run(y, feed_dict={v=np.ones(10)})  # Run the graph\n",
        "```\n",
        "where `feed_dict` is how we \"feed\" input (NumPy arrays) to TensorFlow, and `y_value` will be a NumPy array containing the result of the computation.\n",
        "\n",
        "This seems clunky for such a simple example - but it will dramatically simplify things when we start working with more complicated models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 0: Simple Adder\n",
        "\n",
        "Open [`graph.py`](graph.py).  This file contains a number of class/function placeholders that we will implement through the course of this notebook - as well as a wealth of comments explaining how they work.\n",
        "\n",
        "Implement the methods of the `AddTwo` class using TensorFlow.  In particular:\n",
        "- `__init__` should construct a graph that adds the numbers.  Use two placeholders for the numbers to add.\n",
        "- `Add` should (only) execute the graph with its two arguments and return the result.  It should not create any graph nodes or the session object.\n",
        "\n",
        "When you are done, execute the next cell to test it, and verify that the unit tests below both pass.\n",
        "\n",
        "Be sure that your adder can handle parameters of any dimension! (*Hint: TensorFlow will mostly do this automatically, unless you tell it not to.*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reload(graph)\n",
        "adder = graph.AddTwo()  # Internally, creates tf.Graph and tf.Session\n",
        "print adder.Add(40, 2)\n",
        "print adder.Add([1,2],[3,4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reload(graph)\n",
        "reload(graph_test)\n",
        "unittest.TextTestRunner(verbosity=2).run(\n",
        "    unittest.TestLoader().loadTestsFromName(\n",
        "        'TestAdder.test_adder', graph_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you didn't already, make sure that your adder can handle parameters of any dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reload(graph)\n",
        "reload(graph_test)\n",
        "unittest.TextTestRunner(verbosity=2).run(\n",
        "    unittest.TestLoader().loadTestsFromName(\n",
        "        'TestAdder.test_vector_adder', graph_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Affine & Fully Connected Layers\n",
        "\n",
        "### Brief Review of Machine Learning\n",
        "\n",
        "In supervised learning, parametric models are those where the model is a function of a certain form with a number of unknown _parameters_.  Together with a loss function and a training set, an optimizer can select parameters to minimize the loss with respect to the trainin set.  Common optimizers include stochastic gradient descent.  It tweaks the parameters slightly to move the loss \"downhill\" due to a small batch of examples from the training set.\n",
        "\n",
        "### Linear & Logistic Regression\n",
        "\n",
        "You've likely seen linear regression before.  In linear regression, we fit a line (technically, hyperplane) that predicts a target variable, $y$, based on some features $x$.  The form of this model is affine (even if we call it \"linear\"):  $y_{hat} = xW + b$ where $W$ and $b$ are weights and an offset, respectively, and are the parameters of this parametric model.  The loss function that the optimizer uses to fit these parameters is the squared error ($||\\cdots||_2$) between the prediction and the ground truth in the training set.\n",
        "\n",
        "You've also likely seen logistic regression, which is tightly related to linear regression.  Logistic regression also fits a line - this time separating the positive and negative examples of a binary classifier.  The form of this model is similar: $y_{hat} = \\sigmoid(xW + b)$.  Again $W$ and $b$ are the parameters of this model.The loss function that the optimizer uses to fit these parameters is the cross entropy between the prediction and the ground truth in the training set.\n",
        "\n",
        "This pattern of an affine transform, $xW + b$, occurs over and over in machine learning.\n",
        "\n",
        "In all of these cases, the optimizer needs:\n",
        "\n",
        "- A batch of examples of $x$ and $y$ from the training set\n",
        "- Variables to maintain the current values of the parameters of the model\n",
        "- A loss function\n",
        "- An optimization strategy\n",
        "\n",
        "### Your Task\n",
        "\n",
        "In this section, you don't need to create the graph and session (we've done it for you).  Instead, you will simply implement functions that construct parts of a larger graph.\n",
        "\n",
        "You will first build an affine layer: $z = xW + b$, and then then a stack of fully connected layers (described in more detail below), each implementing $h = f(xW + b)$.  You'll use the former as a building block for the latter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1(a): Affine Layer\n",
        "In particular, your function will accept a TensorFlow Op that represents the value of $x$ and should return value $z$ of desired dimension.  You must construct whatever variables you need.\n",
        "\n",
        "Implement `affine_layer(...)`.\n",
        "\n",
        "Hints:\n",
        "- use `tf.get_variable()` to create variables to store the current values of parameters.\n",
        "- `W` should be randomly initialized using [Xavier initialization](https://www.tensorflow.org/versions/master/api_docs/python/contrib.layers/initializers)\n",
        "- `b` should be initialized to a vector of zeros\n",
        "- `a * b` is a element-wise product.  Look for the function that performs matrix products!\n",
        "\n",
        "Run the little fragment below until you get your code up and running, then run the more comprehensive unit tests in the cell below that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reload(graph)\n",
        "with tf.Graph().as_default():\n",
        "    sess = tf.Session()\n",
        "    x_ph = tf.placeholder(tf.float32, shape=(None, 3))\n",
        "    y = graph.affine_layer(1, x_ph)  #### <---- Your code called here.\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    print 'You should have two trainable variables, one for each of parameters W and b: ', len(tf.trainable_variables())\n",
        "    assert len(tf.trainable_variables()) == 2\n",
        "\n",
        "    print 'These should be a (3, 1) W weight matrix and a (1,) offset.'\n",
        "    variables = sess.run(tf.trainable_variables())\n",
        "    print variables[0].shape\n",
        "    print variables[1].shape\n",
        "    assert set([variables[0].shape, variables[1].shape]) == set([(3, 1), (1,)])\n",
        "\n",
        "    print 'This should be [[3.888]].'\n",
        "    y_val = sess.run(y, feed_dict={x_ph: np.array([[1, 2, 3]])})\n",
        "    print y_val\n",
        "    assert y_val.shape == (1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reload(graph)\n",
        "reload(graph_test)\n",
        "unittest.TextTestRunner(verbosity=2).run(\n",
        "    unittest.TestLoader().loadTestsFromName(\n",
        "        'TestLayer.test_affine', graph_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1(b): Fully-Connected Layers\n",
        "\n",
        "A fully connected layer has the following form (you'll notice this is very similar to logistic regression!):\n",
        "\n",
        "1.  An affine transform\n",
        "2.  An elementwise nonlinearity $f(z)$ (this is sigmoid in logistic regression; we'll use [relu](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) here, but the idea is the same)\n",
        "\n",
        "These fully connected layers, in square brackets below, can be stacked repeatedly to build a deep neural network:\n",
        "\n",
        "$x \\rightarrow [xW + b \\rightarrow z \\rightarrow f(z)] \\rightarrow h_1 \\rightarrow [h_1W + b \\rightarrow z1 \\rightarrow f(z_1)] \\rightarrow h_2 \\cdots$\n",
        "\n",
        "- Implement the `fully_connected_layers()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reload(graph)\n",
        "reload(graph_test)\n",
        "unittest.TextTestRunner(verbosity=2).run(\n",
        "    unittest.TestLoader().loadTestsFromName(\n",
        "        'TestLayer.test_fully_connected_layers', graph_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: Training a Neural Network\n",
        "\n",
        "Let's put it all together, and build a simple neural network that fits some training data.\n",
        "\n",
        "- Implement the `train_nn()` function.\n",
        "\n",
        "**Note:** you will need to do all the work (creating the graph and the session and a training op).\n",
        "\n",
        "To get the tests to pass, please use [`tf.train.GradientDescentOptimizer`](https://www.tensorflow.org/api_docs/python/train/optimizers#GradientDescentOptimizer) as your optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reload(graph_test)\n",
        "X_train, y_train, X_test, y_test = graph_test.generate_data(1000, 10)\n",
        "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap='bwr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Hint:** You should expect to see an initial loss here of 0.2 - 1.0.  This is because a well-initialized random classifier tends to output a uniform distribution.  For each example in the batch, we either compute the cross-entropy loss of the label ([1, 0] or [0, 1]) against the model's output (~[0.5, 0.5]).  Both cases result in -ln(0.5) = ln(2) = 0.69.\n",
        "\n",
        "Of course, your random classifier won't output exactly uniform distributions (it's random after all), but you should anticipate it being pretty close.  If it's not, your initialization may be broken and make it hard for your network to learn.\n",
        "\n",
        "**[Optional]** Some technical details... if your randomly initialized network is outputting very confident predictions, the loss computed may be very large while at the same time the sigmoids in the network are likely in saturation, quickly shrinking gradients.  The result is that you make tiny updates in the face of a huge loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "reload(graph)\n",
        "reload(graph_test)\n",
        "unittest.TextTestRunner(verbosity=2).run(\n",
        "    unittest.TestLoader().loadTestsFromName(\n",
        "        'TestNN.test_train_nn', graph_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That was fairly straightforward...  the data is clearly linearly separable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tuning Parameters\n",
        "\n",
        "Let's try our network on a problem that's a bit harder!\n",
        "\n",
        "Here, we'll train a neural network with a couple of hidden layers before the final sigmoid.  This lets the network learn non-linear decision boundaries.\n",
        "\n",
        "Try playing around with the hyperparameters to get a feel for what happens if you set the learning rate too big (or too small), or if you don't give the network enough capacity (i.e. hidden layers and width)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reload(graph_test)\n",
        "X_train, y_train, X_test, y_test = graph_test.generate_non_linear_data(1000, 10)\n",
        "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap='bwr')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hidden_layers = [10, 10]\n",
        "batch_size = 50\n",
        "epochs = 2000\n",
        "learning_rate = 0.001\n",
        "predictions = graph.train_nn(X_train, y_train, X_test, hidden_layers, batch_size, epochs, learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.scatter(X_test[:,0], X_test[:,1], c=predictions, cmap='bwr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That looks pretty good!\n",
        "\n",
        "Let's compare the predictions vs. the labels and see what we got wrong..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.scatter(X_test[:,0], X_test[:,1], c=(predictions==y_test), cmap='bwr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Only a tiny number of errors (hopefully!).  Good work!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Congratulations\n",
        "\n",
        "You have implemented a deep neural network using tensorflow!\n",
        "\n",
        "One remaining API you may want to take a look at is [tf.nn.embedding_lookup](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#embedding_lookup).  It is simply an op that takes a variable (like the \"w\" you did in your affine layer) and returns a column from it.  This will be useful later when we \"embed\" words into vector space.  We'll have our embedding table as a single variable with dimensions `[#words x word_vector_length]` and we'll use this op to select word vectors from it efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
